amber
amber <-  as(ratings, "sparseMatrix")
min(rowCounts(amber))
finalRatings <- as(ratings, 'realRatingMatrix')
min(rowCounts(finalRatings))
finalRatings@Dimnames[1] <- list(as.character(1:nrow(ratings)))
View(finalRatings)
min(rowCounts(finalRatings)) - 5
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 5
nFold <- 5
c(1, 5, seq(10, 100, 10))
seq(5, 20, 5)
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 5
nFold <- 5
# define evaluation scheme
evalScheme <- evaluationScheme(data = finalRatings, method = "crossvalidation", k = n_fold, given = toKeep, goodRating = ratingThreshold)
library(tidyverse)
library(pander)
library(knitr)
library(dplyr)
library(recommenderlab)
library(readxl)
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 5
nFold <- 5
# define evaluation scheme
evalScheme <- evaluationScheme(data = finalRatings, method = "crossvalidation", k = n_fold, given = toKeep, goodRating = ratingThreshold)
# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, method = "crossvalidation", k = n_fold, given = toKeep, goodRating = ratingThreshold)
# load libraries
library(tidyverse)
library(pander)
library(knitr)
library(dplyr)
library(recommenderlab)
library(readxl)
jester <- read_xls("jester-data-1.xls", col_names = FALSE)
colnames(jester) <- c("ratingCount", seq(100))
row.names(jester) <- 1:nrow(jester)
# remove num jokes column
ratings <- jester[-1]
# replace 0 (no rating) with NULL
ratings[ratings == 99] <- NA
ratings <- as.matrix(ratings)
finalRatings <- as(ratings, 'realRatingMatrix')
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 5
nFold <- 5
# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, method = "crossvalidation", k = n_fold, given = toKeep, goodRating = ratingThreshold)
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 5
nFold <- 5
# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, method = "crossvalidation", k = nFold, given = toKeep, goodRating = ratingThreshold)
evaluationScheme(finalRatings, method="crossvalidation", k = nFold, given=toKeep, goodRating = ratingThreshold)
evalScheme <- evaluationScheme(finalRatings, method = "cross-validation", k = nFold, given = toKeep, goodRating = ratingThreshold)
evalModels <- list(
IBCF_cos = list(name = "IBCF", param = list(method =
"cosine")),
IBCF_cor = list(name = "IBCF", param = list(method =
"pearson")),
UBCF_cos = list(name = "UBCF", param = list(method =
"cosine")),
UBCF_cor = list(name = "UBCF", param = list(method =
"pearson"))
)
# number of recommendations
nRecs <- c(1, seq(5, 20, 5))
finalResults <- evaluate(x = evalScheme, method = evalModels, n = nRecs)
lapply(finalResults, avg)
plot(finalResults, annotate = 1, legend = "topleft") title("ROC
plot(finalResults, annotate = 1, legend = "topleft")
plot(finalResults, annotate = 1, legend = "topleft", title ="ROC curve")
plot(finalResults, annotate = 1, legend = "topleft", main ="ROC curve")
avgs <- lapply(finalResults, avg)
avgs
plot(finalResults, annotate = 1, legend = "topleft", main ="ROC curve")
View(finalRatings)
View(finalRatings@data)
View(ratings)
colMeans(ratings)
colMeans(ratings, na.rm = TRUE)
plot(finalResults, annotate = 1, legend = "topleft", main ="ROC curve")
plot(finalResults, annotate = 1, legend = "topleft", title ="ROC curve")
plot(finalResults, annotate = 1, legend = "topleft", main ="ROC curve")
plot(finalResults, main ="ROC curve", annotate = 1, legend = "topleft")
plot(finalResults, annotate = 1, legend = "topleft")
title("ROC curve")
avgs <- lapply(finalResults, avg)
avgs
5.3+14.67+5.39+43.61
4.5+10.48+6.2+47.79
0.4418084+0.5580316+10.276455+57.72370
sapply(evalScheme@runsTrain, length)
plot(finalResults, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 2
nFold <- 3
# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, method = "cross-validation", k = nFold, given = toKeep, goodRating = ratingThreshold)
evalModels <- list(
IBCF_cos = list(name = "IBCF", param = list(method =
"cosine")),
IBCF_cor = list(name = "IBCF", param = list(method =
"pearson")),
UBCF_cos = list(name = "UBCF", param = list(method =
"cosine")),
UBCF_cor = list(name = "UBCF", param = list(method =
"pearson"))
)
# number of recommendations
nRecs <- c(1, seq(5, 20, 5))
finalResults <- evaluate(x = evalScheme, method = evalModels, n = nRecs)
avgs <- lapply(finalResults, avg)
avgs
finalResults
finalResults$UBCF_cor
finalResults$UBCF_cor[1]
finalResults$UBCF_cor[,]
finalResults$UBCF_cor
avg(finalResults)
plot(finalResults, annotate = 1, legend = "topleft")
title("ROC curve")
plot(finalResults, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
finalRec <- Recommender(getData(finalRatings, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
Recommender(getData(finalRatings@data, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
set.seed(200)
e <- evaluationScheme(finalRatings, method="split", train= trainPct, given= toKeep, goodRating = ratingThreshold)
finalRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
predictions <- predict(finalRec, getData(e, "known"), type="ratings")
View(ratings)
View(ratings[1:5000,])
View(predictions)
accuracy <- calcPredictionAccuracy(predictions, getData(e, "unknown"))
accuracy
jester <- read_xls("jester-data-1.xls", col_names = FALSE)
colnames(jester) <- c("ratingCount", seq(100))
row.names(jester) <- 1:nrow(jester)
# remove num jokes column
ratings <- jester[-1]
# replace 0 (no rating) with NULL
ratings[ratings == 99] <- NA
ratings <- ratings[1:5000]
jester <- data.frame(read_xls("jester-data-1.xls", col_names = FALSE))
colnames(jester) <- c("ratingCount", seq(100))
row.names(jester) <- 1:nrow(jester)
ratings <- jester[-1]
ratings[ratings == 99] <- NA
ratings <- ratings[1:5000]
View(ratings)
View(ratings[1:5000,])
ratings <- ratings[1:5000,]
ratings <- as.matrix(ratings)
View(ratings)
finalRatings <- as(ratings, 'realRatingMatrix')
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 2
nFold <- 3
# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, method = "cross-validation", k = nFold, given = toKeep, goodRating = ratingThreshold)
evalModels <- list(
IBCF_cos = list(name = "IBCF", param = list(method =
"cosine")),
IBCF_cor = list(name = "IBCF", param = list(method =
"pearson")),
UBCF_cos = list(name = "UBCF", param = list(method =
"cosine")),
UBCF_cor = list(name = "UBCF", param = list(method =
"pearson"))
)
# number of recommendations
nRecs <- c(1, seq(5, 20, 5))
finalResults <- evaluate(x = evalScheme, method = evalModels, n = nRecs)
avgs <- lapply(finalResults, avg)
avgs
plot(finalResults, annotate = 1, legend = "topleft")
title("ROC curve")
plot(finalResults, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
set.seed(200)
e <- evaluationScheme(finalRatings, method="split", train= trainPct, given= toKeep, goodRating = ratingThreshold)
finalRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
predictions <- predict(finalRec, getData(e, "known"), type="ratings")
# calc accuracy on test set
accuracy <- calcPredictionAccuracy(predictions, getData(e, "unknown"))
accuracy
predictions
predictions@data
View(predictions@data)
predictions@items
predictions <- predict(finalRec, getData(e, "known"), n = 10)
predictions@items[1]
accuracy <- calcPredictionAccuracy(predictions, getData(e, "unknown"))
accuracy <- calcPredictionAccuracy(predictions, getData(e, "unknown"), goodRating = 2)
accuracy <- calcPredictionAccuracy(predictions, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
accuracy
accuracy$precision
accuracy
accuracy['precision']
predictions
predictions@itemLabels
predictions@n
predictions@ratings
predictions
predictions@items[1]
str(predictions@items[1])
predictions@items[1][1]
predictions@items[1][[1]]
predictions@items[1][[1]][1]
test <- predictions@items[1][1]
str(test)
test
test[1]
test[1][1]
test[1][1][1]
predictions@items[1][[1]][1]
predictions@items[1][[1]]
predictions@items[1][[1]]
test <- predictions@items[1][[1]]
test
test[1]
set(test)
range(1,100)
seq(1:100)
seq(1:100) in 1
1 %in% seq(1:100)
test %in% seq(1:100)
test !%in% seq(1:100)
test %!in% seq(1:100)
seq(1:100) %in% test
seq(1:100)[seq(1:100) %in% test]
test
seq(1:100)[!seq(1:100) %in% test]
test2 <- seq(1:numJokes)[!seq(1:numJokes) %in% test]
numJokes <- 100
test2 <- seq(1:numJokes)[!seq(1:numJokes) %in% test]
test2
sample(test2,1)
sample(test2,2)
test[8:10]
test[9:10]
test
test[9:10] <- sample(test2,2)
test
rapply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
return(x)
}, how='list')
test
rapply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
}, how='list')
str(test)
c(test)
str(c(test))
apply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
}, how='list')
apply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
})
lapply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
})
rapply(test, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
})
rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
})
test3 <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
}, how = 'list')
test3
View(test3)
test3 <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
return(x)
}, how = 'list')
test3 <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
print(x)
return(x)
}, how = 'list')
test3 <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
return(x)
}, how = 'list')
View(test3)
predictions@items[1]
test3[1]
predictions@newItems <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
return(x)
}, how = 'list')
newPredictions <- predictions
newPredictions@items <- rapply(predictions@items, function(x){
baseList <- seq(1:numJokes)[!seq(1:numJokes) %in% x]
replacements <- sample(baseList,2)
x[9:10] <- replacements
return(x)
}, how = 'list')
View(newPredictions)
accuracy <- calcPredictionAccuracy(newPredictions, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
accuracy['precision']
View(ratings)
finalRec@model
evalModels <- list(
IBCF_cos = list(name = "IBCF", param = list(method =
"cosine")),
IBCF_cor = list(name = "IBCF", param = list(method =
"pearson")),
UBCF_cos = list(name = "UBCF", param = list(method =
"cosine")),
UBCF_cor = list(name = "UBCF", param = list(method =
"pearson")),
POPULAR = list(name = "POPULAR", param = NULL)
)
# number of recommendations
nRecs <- c(1, seq(5, 20, 5))
finalResults <- evaluate(x = evalScheme, method = evalModels, n = nRecs)
avgs <- lapply(finalResults, avg)
avgs
plot(finalResults, annotate = 1, legend = "topleft")
title("ROC curve")
plot(finalResults, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
popRec <- Recommender(getData(e, "train"), 'POPULAR')
popPred <- predict(popRec, getData(e, "known"), n = 10)
# calc accuracy on test set
popAcc <- calcPredictionAccuracy(popPred, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
popAcc['precision']
# UBCF Model
ubcfRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
ubcfPred <- predict(ubcfRec, getData(e, "known"), n = 10)
# calc accuracy on test set
ubcfAcc <- calcPredictionAccuracy(ubcfPred, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
ubcfAcc['precision']
popPred[1][[1:2]]
popPred@items[1]
popPred@items[2]
popPred@items[3]
set.seed(200)
e <- evaluationScheme(finalRatings, method="split", train= trainPct, given= toKeep, goodRating = ratingThreshold)
# UBCF Model
ubcfRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
ubcfPred <- predict(ubcfRec, getData(e, "known"), n = 5)
# calc accuracy on test set
ubcfAcc <- calcPredictionAccuracy(ubcfPred, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
ubcfAcc['precision']
ubcfAcc
ubcfPred2 <- predict(ubcfRec,getData(e,'unknown'), type = 'ratings')
ubcfAcc2 <- calcPredictionAccuracy(ubcfPred2, getData(e, "unknown"))
ubcfAcc
ubcfAcc2
ubcfPred2
ubcfPred2@data
# UBCF Model
ubcfRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
ubcfPred <- predict(ubcfRec, getData(e, "known"), n = 5)
ubcfPred2 <- predict(ubcfRec,getData(e,'known'), type = 'ratings')
# calc accuracy on test set
ubcfAcc <- calcPredictionAccuracy(ubcfPred, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
ubcfAcc2 <- calcPredictionAccuracy(ubcfPred2, getData(e, "unknown"))
ubcfAcc
ubcfAcc2
getData(e, "unknown")
getData(e,'known')
toKeep
getData(e, "unknown")[1]
getData(e, "unknown")[1][1]
getData(e, "unknown")[1]
colnames(getData(e, "unknown")[1])
getData(e, "train")
finalRatings
getData(e, "known")
getData(e, "unknown")
287048+31000+41331
ubcfPred@items[1]
randRec <- Recommender(getData(e, "train"), 'RANDOM')
randPred <- predict(ubcfRec, getData(e, "known"), n = 5)
randRec <- Recommender(getData(e, "train"), 'RANDOM')
randPred <- predict(randRec, getData(e, "known"), n = 6)
1/5
# Random Model
randRec <- Recommender(getData(e, "train"), 'RANDOM')
hybridRec <- HybridRecommender(ubcfRec,randRec, c(0.8,0.2))
# UBCF Model
ubcfRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
hybridRec <- HybridRecommender(ubcfRec,randRec, weights = c(0.8,0.2))
hybridPredN <- predict(hybridRec, getData(e, "known"), n = 5)
hybridPredR <- predict(hybridRec, getData(e,'known'), type = 'ratings')
randRec <- Recommender(getData(e, "train"), 'RANDOM')
hybridRec <- HybridRecommender(ubcfRec,randRec, weights = c(0.8,0.2))
hybridPredN <- predict(hybridRec, getData(e, "known"), n = 5)
hybridPredR <- predict(hybridRec, getData(e,'known'), type = 'ratings')
# calc accuracy on test set
hybridAccN <- calcPredictionAccuracy(hybridPredN, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
hybridAccR <- calcPredictionAccuracy(hybridPredR, getData(e, "unknown"))
hybridAccN
hybridAccR
ubcfAccN
set.seed(200)
e <- evaluationScheme(finalRatings, method="split", train= trainPct, given= toKeep, goodRating = ratingThreshold)
# UBCF Model
ubcfRec <- Recommender(getData(e, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 100, normalize = 'center'))
ubcfPredN <- predict(ubcfRec, getData(e, "known"), n = 5)
ubcfPredR <- predict(ubcfRec,getData(e,'known'), type = 'ratings')
# calc accuracy on test set
ubcfAccN <- calcPredictionAccuracy(ubcfPredN, getData(e, "unknown"), given = toKeep, goodRating = ratingThreshold)
ubcfAccR <- calcPredictionAccuracy(ubcfPredR, getData(e, "unknown"))
ubcfAccN
ubcfAccR
seq(5:100,by=5)
seq(5,100,10)
seq(0,100,10)
seq(10,100,10)
numNeighbors <- seq(10,100,10)
ubcfModels <- lapply(numNeighbors, function(n){
list(name = "UBCF", param = list(method = "pearson", nn = n))
})
uubcfModels
ubcfModels
numNeighbors <- seq(10,100,10)
ubcfModels <- lapply(numNeighbors, function(n){
list(name = "UBCF", param = list(method = "pearson", nn = n))
})
names(ubcfModels) <- paste0("UBCF_", numNeighbors)
ubcfSchemes <- evaluate(x = evalScheme, method = ubcfModels, n = 5)
numNeighbors <- seq(20,100,20)
ubcfModels <- lapply(numNeighbors, function(n){
list(name = "UBCF", param = list(method = "pearson", nn = n))
})
names(ubcfModels) <- paste0("UBCF_", numNeighbors)
ubcfSchemes <- evaluate(x = evalScheme, method = ubcfModels, n = 5)
plot(ubcfSchemes, "prec/rec", annotate = 1, legend = "bottomright")
ubcfSchemes
avg(ubcfSchemes)
barplot(ubcfSchemes, "prec/rec", annotate = 1, legend = "bottomright")
avg(ubcfSchemes)
numNeighbors <- seq(100,200,50)
ubcfModels <- lapply(numNeighbors, function(n){
list(name = "UBCF", param = list(method = "pearson", nn = n))
})
names(ubcfModels) <- paste0("UBCF_", numNeighbors)
ubcfSchemes <- evaluate(x = evalScheme, method = ubcfModels, n = 5)
avg(ubcfSchemes$precision)
avg(ubcfSchemes)
# UBCF Model
ubcfRec <- Recommender(getData(evalScheme, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 200, normalize = 'center'))
ubcfPredN <- predict(ubcfRec, getData(evalScheme, "known"), n = 5)
ubcfPredR <- predict(ubcfRec,getData(evalScheme,'known'), type = 'ratings')
# calc accuracy on test set
ubcfAccN <- calcPredictionAccuracy(ubcfPredN, getData(evalScheme, "unknown"), given = toKeep, goodRating = ratingThreshold)
ubcfAccR <- calcPredictionAccuracy(ubcfPredR, getData(evalScheme, "unknown"))
ubcfAccN
ubcfAccR
# Random Model
randRec <- Recommender(getData(evalScheme, "train"), 'RANDOM')
hybridRec <- HybridRecommender(ubcfRec,randRec, weights = c(0.8,0.2))
hybridPredN <- predict(hybridRec, getData(evalScheme, "known"), n = 5)
hybridPredR <- predict(hybridRec, getData(evalScheme,'known'), type = 'ratings')
# calc accuracy on test set
hybridAccN <- calcPredictionAccuracy(hybridPredN, getData(evalScheme, "unknown"), given = toKeep, goodRating = ratingThreshold)
hybridAccR <- calcPredictionAccuracy(hybridPredR, getData(evalScheme, "unknown"))
hybridAccN
hybridAccR
hybridAccR['RMSE']
hybridAccN['precision']
data.frame(METHOD = c('UBCF','HYBRID'),
PRECISION = c(ubcfAccN['precision'], hybridAccN['precision']),
RMSE = c(ubcfAccR['RMSE'], hybridAccR['RMSE']))
hybridPredN@items[1]
ubcfPredN@items[1]
hybridPredN@items[2]
ubcfPredN@items[2]
