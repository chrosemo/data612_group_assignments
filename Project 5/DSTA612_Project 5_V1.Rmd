---
title: "DATA612_Project 5 | IMPLEMENTING A RECOMMENDER SYSTEM ON SPARK"
author: "Amber Ferger, Charlie Rosemond, Juanelle Marks" 
date: "6/14/2020"
output: 
    html_document:
     theme: lumen
     highlight: tango
     toc: TRUE
---
## Assignment
The goal of this project is give you practice beginning to work with a distributed recommender system. It is sufficient for this assignment to build out your application on a single node.

Adapt one of your recommendation systems to work with Apache Spark and compare the performance with your previous iteration. Consider the efficiency of the system and the added complexity of using Spark. You may complete the assignment using PySpark (Python), SparkR (R) , sparklyr (R), or Scala.

Please include in your conclusion: For your given recommender systemâ€™s data, algorithm(s), and (envisioned) implementation, at what point would you see moving to a distributed platform such as Spark becoming necessary?

You may work on any platform of your choosing, including Databricks Community Edition or in local mode. You are encouraged but not required to work in a small group on this project.


## Motivation
In this assignment, we create two alternating least squares (ALS) recommenders--one with *recommenderlab* and one with Apache Spark via *sparklyr*--and compare their performance. Our goal is to gain experience with Spark and begin to assess its advantages over a non-distributed platform.


## Data
Our data source is the [Jester](http://eigentaste.berkeley.edu/dataset/) dataset, which contains data from 24,983 users who have rated 36 or more jokes. Ratings are real values ranging from -10.00 to +10.00 (the value "99" corresponds to "null" = "not rated"). Each row in the dataset represents a different user, and the first column represents the total number of jokes the individual has rated. The remaining 100 columns give the ratings for each joke. 


### Load libraries

```{r message=FALSE, include=FALSE}
# load libraries
library(tidyverse)
library(pander)
library(knitr)
library(dplyr)
library(recommenderlab)
library(readxl)
library(sparklyr)
```

### Read and prepare data
We start by reading in the jester data and replacing the current null values ("99") with median ratings for each joke. We then subset to the first 5000 users and prepare a matrix for the recommender.

```{r, warning=FALSE, message=FALSE}
# Read jester data
jester <- data.frame(read_xls("jester-data-1.xls", col_names = FALSE))
colnames(jester) <- c("ratingCount", seq(100))
row.names(jester) <- 1:nrow(jester)
ratings <- jester[-1] # remove joke count column

# Replace 99 (no rating) with NA
ratings[ratings == 99] <- NA

# Function to replace NA with column medians
medValue.f <- function(x){
   x[is.na(x)] = median(x, na.rm=TRUE) 
   return(x)
}
ratings <- apply(ratings, 2, medValue.f)

# Subset to first 5000 users
ratings <- ratings[1:5000,]
```

```{r}
# Create large dgCMatrix
ratings_matrix <- as.matrix(ratings)
recclab_ratings <- as(ratings_matrix, 'realRatingMatrix')
```

## Exploratory data analysis


## Recommenders
We compare ALS recommenders built using *recommenderlab* and Spark via *sparklyr*.

### recommenderlab
We start with *recommenderlab*. We split the ratings matrix into train (0.8) and test (0.2) sets; for test users, give 15 joke items; and set a good rating as 2 given the mean and median.

```{r}
# Defining an evaluation scheme
set.seed(612)
trainPct <- 0.8
toKeep <- 15
ratingThreshold <- 2
# define evaluation scheme
e <- evaluationScheme(recclab_ratings, method = "split", train = trainPct, given = toKeep, goodRating = ratingThreshold)
```

Next, we train our ALS recommender, setting 10 latent factors and 10 iterations, and use it to predict ratings. Prediction takes a relatively substantial length of time (approximately 5 minutes).

```{r}
recclab_recc <- Recommender(getData(e, "train"), method = "ALS", param = list(n_factors = 10, lambda = 0.1, n_iterations = 10, seed = 612))
```

```{r}
start <- Sys.time()
recclab_pred <- predict(recclab_recc, getData(e, "known"), type="ratings")
stop <- Sys.time()
```

```{r}
(recclab_error <- calcPredictionAccuracy(recclab_pred, getData(e, "unknown")))
(recclab_time <- stop - start)
```


### sparklyr
Next, we move to Spark and *sparklyr*. We start by restructuring the ratings data to facilitate modeling.

```{r}
# function to replace NA with the median of the column
ratings_df <- as.data.frame(ratings)
```

```{r}
ratings_df$user <- 1:nrow(ratings_df)
ratings_df <- ratings_df %>% gather("item", "rating", -user)
ratings_df$item <- as.numeric(ratings_df$item)
ratings_df$rating <- as.numeric(ratings_df$rating)
ratings_df <- arrange(ratings_df, user, item)
```


Our work in Spark starts with establishing a local connection.

```{r}
sc <- spark_connect(master = "local", spark_home = "C:\\Spark\\spark-2.4.6-bin-hadoop2.7")
```


We create a tbl_spark object containing the long ratings dataframe.

```{r}
sparklyr_ratings <- sdf_copy_to(sc, ratings_df)
```


We then build and make predictions using an ALS recommender with parameters that mirror those of the model built with *recommenderlab*.

```{r}
sparklyr_recc <- ml_als(sparklyr_ratings, rating ~ user + item, rank = 10, reg_param = 0.1, max_iter = 10)
```

```{r}
start <- Sys.time()
sparklyr_pred <- ml_predict(sparklyr_recc, sparklyr_ratings)
stop <- Sys.time()
```

```{r}
(sparklyr_error = c(
  "RMSE" = RMSE(sparklyr_df$rating, sparklyr_df$prediction),
  "MSE" = MSE(sparklyr_df$rating, sparklyr_df$prediction),
  "MAE" = MAE(sparklyr_df$rating, sparklyr_df$prediction)
  )
)
(sparklyr_time <- stop - start) 
```


## Discussion

## Conclusion

## Reference
