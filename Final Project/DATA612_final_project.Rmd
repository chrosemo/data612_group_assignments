---
title: "DATA612 Final Project | Jester Dataset"
author: "Amber Ferger, Charlie Rosemond, Juanelle Marks" 
date: "7/1/2020"
output: 
    html_document:
     theme: lumen
     highlight: tango
     toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries
```{r message=FALSE, include=FALSE}
# load libraries
library(tidyverse)
library(dplyr)
library(readxl)
library(sparklyr)
library(recommenderlab)

#library(stringr)
library(tm)
library(tidyr)
library(tidytext)

# install a local version of Spark for development purposes
# spark_install()
```

## Project Goals ****UPDATE TO REFLECT SHIFT TO HYBRID****
We will build a recommender system that predicts Jester users’ joke ratings. We will use a cloud platform, Databricks Community Edition, for processing since the intended dataset to be used is large. Our intended recommender system  will be based on a  hybrid of the content-based approach and IBCF model. Text analysis will be performed on the raw jokes text in order to elicit features that will be used in the design and development of the content based recommender. Additionally, a binarised matrix will be created from jokes ratings from which a IBCF recommender will be developed. We will employ various evaluation methods to select our final model. ****TBC*** "  Are we doing monolithic hybrid? where two approaches are ingtegrated into one algorithm - for example in this case the algorithm takes into account ratings AND item description(its sentiment score)

## About the Dataset ****APPEND TO REFLECT SHIFT TO HYBRID****
The Jester datset 3 will be used in this project. Jester is a joke recommender system developed by the University of California, Berkeley Laboratory of Automation Science and Engineering. The Jester dataset contains millions of joke ratings made by users of the recommender system, which learns from new users. Our chosen dataset (several Jester sets of different sizes are available) initially describes almost 55,000 users making over 1.8 million ratings of 150 different jokes. The Jester team collected these ratings from April 1999 to May 2003 and November 2006 to March 2015. Ratings are real values ranging from -10.00 to +10.00 (the value "99" corresponds to "null" = "not rated"). Each row in the dataset represents a different user, and the first column represents the total number of jokes the individual has rated. The remaining 100 columns give the ratings for each joke. (Learn more about Jester [here](http://eigentaste.berkeley.edu/about.html).) The joke text included in the *jokeText* file will also be used so as to facilitate features engineering.


## Data Preprocessing

### Read and prepare data

First, we will load in the jester data set. We can see from this that we have $54,905$ users and $150$ jokes (plus one column for the rated joke count).
```{r}
# Downloading ratings data to tempfile
dl <- tempfile()
download.file("http://eigentaste.berkeley.edu/dataset/JesterDataset3.zip", dl)

# Unzipping then reading into a tibble
con <- unzip(dl, "FINAL jester 2006-15.xls")
working <- data.frame(read_xls(con, col_names = FALSE))

dim(working)
```

We'll start by removing the count column and retired jokes (jokes that weren't rated). After this cleansing, we're left with a total of $128$ jokes. 
```{r}
# Removing count column
ratings_working <- working[-1]

# Adding column and row names
names(ratings_working) <- 1:dim(ratings_working)[2]
row.names(ratings_working) <- 1:nrow(ratings_working)

# Removing retired jokes (columns)
retired <- c("1","2","3","4","5","6","9","10","11","12","14","20","27","31","43","51","52","61","73","80","100","116")  # per Jester website -- jokes removed and thus not rated
ratings_working[, retired] <- list(NULL)

dim(ratings_working)
```

The raw data represents non-rated jokes as the number 99, so we will replace these values with nulls. We'll also create a binary ratings matrix that can be used for our recommender system. A $0$ will represent a negative rating or a joke that hasn't been rated and a $1$ will represent a positively rated joke. 

```{r}
# Replacing "99" with NA
ratings_working[ratings_working == 99] <- NA
ratings_working <- as.matrix(ratings_working)

# Create large dgCMatrix
finalRatings <- as(ratings_working, 'realRatingMatrix')
```

## Exploratory Data Analysis of Real Ratings

### Distribution of joke counts

```{r}

jokeCount <- rowCounts(finalRatings)
hist(jokeCount,
     main = ' Distribution of Number of Jokes Rated per User',
     xlab = 'Number of Jokes Rated',
     ylab = 'Number of Users',
      col = 'lightblue')
```

### Average rating across jokes
```{r}
mean_rating <- colMeans(finalRatings, na.rm = T)
plot(quantile(mean_rating), main = 'Quantiles of Average Joke Ratings', type = 'l', col = 'red')
```


## Binarising the ratings

Given that the intended recommender  will hybrid in nature, we decided to binarise the ratings so as to be able to seamlessly integrate it with the outputs of the feature engineering on the raw 'joke texts'.

```{r}
# Creating binary matrix where positive ratings (> 0) are 1 and all other ratings or NA are 0
ratings_binary <- ifelse(is.na(ratings_working) | ratings_working <= 0, 0, 1)

# Creating binary rating matrix
ratings <- as(ratings_binary, "binaryRatingMatrix")
```

### Visualisation of Binarised Ratings
```{r}
# An image of the binary rating matrix
image(ratings[1:50, 1:50], main = "Viz of Binary Rating Matrix")
```
The visualisation of binarised ratings shows that a number of jokes (represented by the white spaces) were not rated or were rated below 0.

### Number of ratings per joke

```{r}
# Depicting the number of ratings per joke
n_ratings <- colCounts(ratings)
hist(n_ratings, main = "Number of ratings per joke")
```

## Feature Engineering to support content-based approach
The first thing we will do is create features for our jokes. We will base our feature development on research by Rada Mihalcea and Stephen Pulman.

### Joke Characteristics

### Joke Text

The joke text is included in the *jokeText* file, so we will import this into our session. Let's create a few features from the joke text:

* **JOKE_TYPE**: There are 2 general types of jokes - (1) Q&A (ex: Q. What's the difference between a man and a toilet?   A. A toilet doesn't follow you around after you use it.) and (2) story-line jokes (ex: A man visits the doctor. The doctor says "I have bad news for you.You have cancer and Alzheimer's disease".  The man replies "Well,thank God I don't have cancer!) We will classify Q&A jokes as $1$ and story-line jokes as $0$.
* **JOKE_LENGTH**: The number of words in the joke. 
* **EXCITED_COUNT**: The number of times an exclamation point is used in a given joke. 
* **SENTIMENT_SCORE**: We will analyze the general sentiment of the joke using the the lexicon of Bing Liu and collaborators.

```{r message=FALSE} 

# https://www.cs.ox.ac.uk/files/244/mihalcea.cicling07.pdf
jokeText <- data.frame(read_xls("jokeText.xls", col_names = FALSE))
colnames(jokeText) <- c('JOKE_TEXT')
jokeText$JOKE_NUM <- 1:nrow(jokeText)

# JOKE_TYPE, JOKE_LENGTH, EXCITED_COUNT columns
jokeText <- jokeText %>%
  mutate(JOKE_TYPE = ifelse(str_detect(JOKE_TEXT, 'Q:') == 'TRUE', 1, 0),
         JOKE_LENGTH = str_count(JOKE_TEXT, '\\w+'),
         EXCITED_COUNT = str_count(JOKE_TEXT, '!'))

```

### Joke Sentiment
To analyze the joke sentiment, we'll create a corpus for our joke text using the *tm* package. We'll remove whitespace, stopwords, and punctuation and also transform the text to lowercase. This will allow us to create a *DocumentTermMatrix* which identifies the counts of words in each joke.  
  
When we inspect the first few elements of the *DocumentTermMatrix*, we can that it is very sparse: 99% of the counts are 0. This means that there are not a lot of common words used in all jokes.
```{r}
# https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf  
# https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html

# create a corpus of words in the text
jokeCorpus <- VCorpus(VectorSource(jokeText$JOKE_TEXT))

# no whitespace, all lowercase, remove stopwords
jokeCorpus <- tm_map(jokeCorpus, stripWhitespace)
jokeCorpus <- tm_map(jokeCorpus, content_transformer(tolower))
jokeCorpus <- tm_map(jokeCorpus, removeWords, stopwords("english"))
jokeCorpus <- tm_map(jokeCorpus, removePunctuation)

# create document-term matrix
dtm <- DocumentTermMatrix(jokeCorpus)
tm::inspect(dtm)

```

We will transform the *DocumentTermMatrix* into a format that will allow us to analyze the sentiment of each word. We will use the the *get_sentiments()* function, which looks at the words with a positive score from the lexicon of Bing Liu and collaborators.
```{r message=FALSE}

# turn it into a one-term-per-document-per-row data frame
dtm_td <- tidy(dtm)

# sentiments per word
jokeSentiments <- dtm_td %>%
   inner_join(get_sentiments("bing"),by = c(term = "word"))

# sentiments per doc
sentPerDoc <- jokeSentiments %>%
   count(document, sentiment, wt = count) %>%
   spread(sentiment, n, fill = 0) %>%
   mutate(sentiment = positive - negative) %>%
   arrange(sentiment)

# convert to a number
sentPerDoc$document <- as.integer(sentPerDoc$document)

```

We can now join this to our original jokes dataset to get the positive, negative, and total sentiment scores of each joke. We will replace nulls in jokes where no positive or negative words were found with 0 (neutral). We can confirm that the final dataset includes the original features with the sentiment scores. 

```{r}

finalJokes <- left_join(jokeText,sentPerDoc, by = c('JOKE_NUM' = 'document'))
finalJokes[is.na(finalJokes)] <- 0

colnames(finalJokes)

```

### Sentiment Distance Scores
Finally, we'll create a score *category* from the sentiment scores. We'll define 'positive' as anything with a score > 0 and all others as 'non-positive'. We'll then create a matrix of sentiment distance scores - 1 will represent 
```{r}
# Binarizing sentiment
finalJokes$scoreCategory <- ifelse(finalJokes$sentiment > 0, "positive", "non-positive")

# Calculating distance using binary sentiment score
finalJokesTable <- data.table::as.data.table(finalJokes)
dist_score <- finalJokesTable[, 1 - dist(finalJokesTable$scoreCategory == "positive")]

# Converting distance object into matrix
dist_score <- as.matrix(dist_score)
dim(dist_score)  
```


## Recommenders
Our final, hybrid model combines (1) item-based collaborative filtering with (2) the constructed features from the joke text. Each piece will be built out separately and later combined to create the final prediction. The process for building this model is based on Chapter 5 of the course text. 


### Train-Test Split
First, we will split the *binaryRatingMatrix* into training and test sets. The training set will be used to build the model and the test set will be used to calculate the accuracy of predictions. 

```{r}
# Splitting into train/test
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratings), 
                      replace = TRUE, 
                      prob = c(0.8, 0.2))

ratings_train <- ratings[which_train,]
ratings_test <- ratings[!which_train,]
```

### IBCF

Now we can build our initial item-based recommender. We will use Jaccard for distance given binary ratings. 

```{r}
# Training IBCF recommender using Jaccard for distance given binary ratings
ratings_ibcf <- Recommender(data = ratings_train, 
                            method = "IBCF", 
                            parameter = list(method = "Jaccard"))

model_details <- getModel(ratings_ibcf)
```

We'll define a similarity matrix for the jokes, which will give us an idea of how similar jokes are to each other. This matrix is based on the rated jokes -- the more users that rate the jokes positively together, the more similar they are. We can visualise this matrix as well -- the darker colored squares represent jokes that are similar. 

```{r}
# Defining a similarity matrix object
ratings_dist <- as(ratings_ibcf@model$sim, "matrix")

# Visualize similarity matrix
image(ratings_ibcf@model$sim, 
      main = "Similarity matrix",
      xlab = 'Joke Number',
      ylab = 'Joke Number')
```

In the matrix, the score of $0$ represents 1 of 2 things: either the joke is being compared to itself or the two jokes in question are very dissimilar. We can take a look at the distribution of the similarities and see that most items are ranked $0$ or between $0.3$ and $0.4$. 

```{r}
hist(ratings_dist)

```


## Evaluation and Selection

## Conclusion

## Sources

* Mihalcea, Rada, and Stephen Pulman. “Characterizing Humour: An Exploration of Features in Humorous Texts.” Computational Linguistics and Intelligent Text Processing Lecture Notes in Computer Science, 2007, pp. 337–347., doi:10.1007/978-3-540-70939-8_30.

* Usuelli, M., & Gorakala, S. K. (2015). Building a Recommendation System with R. Packt Publishing Limited.
***ADD COURSE TEXT***

