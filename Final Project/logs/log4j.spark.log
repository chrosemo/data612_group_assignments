20/07/15 07:53:16 INFO SparkContext: Invoking stop() from shutdown hook
20/07/15 07:53:16 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
20/07/15 07:53:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/07/15 07:53:16 INFO MemoryStore: MemoryStore cleared
20/07/15 07:53:16 INFO BlockManager: BlockManager stopped
20/07/15 07:53:16 INFO BlockManagerMaster: BlockManagerMaster stopped
20/07/15 07:53:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/07/15 07:53:16 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/07/15 07:53:16 INFO SparkContext: Successfully stopped SparkContext
20/07/15 07:53:16 INFO ShutdownHookManager: Shutdown hook called
20/07/15 07:53:16 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70
20/07/15 07:53:16 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/07/15 07:53:16 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a
20/07/15 07:53:16 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-88134dca-693b-43a2-a27f-c5a1075beb70\userFiles-32d58f01-5412-4b8e-9674-7706dd10c31a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/07/15 07:53:16 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\Temp\1\spark-ae9f5228-545f-441a-b4e0-951a0684e9f6
20/07/15 10:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/07/15 10:48:49 INFO SparkContext: Running Spark version 2.4.3
20/07/15 10:48:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/07/15 10:48:49 INFO SparkContext: Submitted application: sparklyr
20/07/15 10:48:49 INFO SecurityManager: Changing view acls to: y14390
20/07/15 10:48:49 INFO SecurityManager: Changing modify acls to: y14390
20/07/15 10:48:49 INFO SecurityManager: Changing view acls groups to: 
20/07/15 10:48:49 INFO SecurityManager: Changing modify acls groups to: 
20/07/15 10:48:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(y14390); groups with view permissions: Set(); users  with modify permissions: Set(y14390); groups with modify permissions: Set()
20/07/15 10:48:49 INFO Utils: Successfully started service 'sparkDriver' on port 58783.
20/07/15 10:48:49 INFO SparkEnv: Registering MapOutputTracker
20/07/15 10:48:49 INFO SparkEnv: Registering BlockManagerMaster
20/07/15 10:48:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/07/15 10:48:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/07/15 10:48:49 INFO DiskBlockManager: Created local directory at C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-ae5a2f84-f6ef-42f4-8070-88054b54f590
20/07/15 10:48:49 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/07/15 10:48:49 INFO SparkEnv: Registering OutputCommitCoordinator
20/07/15 10:48:49 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/07/15 10:48:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/07/15 10:48:50 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://view-localhost:4040
20/07/15 10:48:50 INFO SparkContext: Added JAR file:/C:/Users/y14390/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://view-localhost:58783/jars/sparklyr-2.4-2.11.jar with timestamp 1594824530029
20/07/15 10:48:50 INFO Executor: Starting executor ID driver on host localhost
20/07/15 10:48:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58804.
20/07/15 10:48:50 INFO NettyBlockTransferService: Server created on view-localhost:58804
20/07/15 10:48:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/07/15 10:48:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, view-localhost, 58804, None)
20/07/15 10:48:50 INFO BlockManagerMasterEndpoint: Registering block manager view-localhost:58804 with 912.3 MB RAM, BlockManagerId(driver, view-localhost, 58804, None)
20/07/15 10:48:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, view-localhost, 58804, None)
20/07/15 10:48:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, view-localhost, 58804, None)
20/07/15 10:48:50 INFO SharedState: loading hive config file: file:/C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
20/07/15 10:48:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
20/07/15 10:48:50 INFO SharedState: Warehouse path is 'C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
20/07/15 10:48:51 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/07/15 10:48:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/07/15 10:48:53 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/07/15 10:48:54 INFO ObjectStore: ObjectStore, initialize called
20/07/15 10:48:54 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/07/15 10:48:54 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/07/15 10:48:55 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/07/15 10:48:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/07/15 10:48:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/07/15 10:48:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/07/15 10:48:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/07/15 10:48:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/07/15 10:48:56 INFO ObjectStore: Initialized ObjectStore
20/07/15 10:48:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/07/15 10:48:57 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/07/15 10:48:57 INFO HiveMetaStore: Added admin role in metastore
20/07/15 10:48:57 INFO HiveMetaStore: Added public role in metastore
20/07/15 10:48:57 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/07/15 10:48:57 INFO HiveMetaStore: 0: get_all_databases
20/07/15 10:48:57 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_all_databases	
20/07/15 10:48:57 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/07/15 10:48:57 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/07/15 10:48:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
20/07/15 10:48:57 INFO SessionState: Created local directory: C:/Users/y14390/AppData/Local/Temp/1/cd9fc4ee-9007-496a-a280-5dd9136c1385_resources
20/07/15 10:48:57 INFO SessionState: Created HDFS directory: C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/y14390/cd9fc4ee-9007-496a-a280-5dd9136c1385
20/07/15 10:48:57 INFO SessionState: Created local directory: C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/cd9fc4ee-9007-496a-a280-5dd9136c1385
20/07/15 10:48:57 INFO SessionState: Created HDFS directory: C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/y14390/cd9fc4ee-9007-496a-a280-5dd9136c1385/_tmp_space.db
20/07/15 10:48:57 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
20/07/15 10:48:57 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:48:57 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:48:58 INFO HiveMetaStore: 0: get_database: global_temp
20/07/15 10:48:58 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/07/15 10:48:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/07/15 10:48:58 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:48:58 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:48:58 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:48:58 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:48:58 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/07/15 10:48:58 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/07/15 10:48:58 INFO SparkContext: Starting job: collect at utils.scala:44
20/07/15 10:48:58 INFO DAGScheduler: Got job 0 (collect at utils.scala:44) with 1 output partitions
20/07/15 10:48:58 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:44)
20/07/15 10:48:58 INFO DAGScheduler: Parents of final stage: List()
20/07/15 10:48:58 INFO DAGScheduler: Missing parents: List()
20/07/15 10:48:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents
20/07/15 10:48:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 912.3 MB)
20/07/15 10:48:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 912.3 MB)
20/07/15 10:48:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on view-localhost:58804 (size: 3.4 KB, free: 912.3 MB)
20/07/15 10:48:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/07/15 10:48:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41) (first 15 tasks are for partitions Vector(0))
20/07/15 10:48:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/07/15 10:48:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7893 bytes)
20/07/15 10:48:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/07/15 10:48:58 INFO Executor: Fetching spark://view-localhost:58783/jars/sparklyr-2.4-2.11.jar with timestamp 1594824530029
20/07/15 10:48:59 INFO TransportClientFactory: Successfully created connection to view-localhost/127.0.0.1:58783 after 23 ms (0 ms spent in bootstraps)
20/07/15 10:48:59 INFO Utils: Fetching spark://view-localhost:58783/jars/sparklyr-2.4-2.11.jar to C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831\fetchFileTemp3445542138183853532.tmp
20/07/15 10:48:59 INFO Executor: Adding file:/C:/Users/y14390/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-92424a26-9103-4be5-8c81-10a541a6f781/userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831/sparklyr-2.4-2.11.jar to class loader
20/07/15 10:48:59 INFO CodeGenerator: Code generated in 231.4801 ms
20/07/15 10:48:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1013 bytes result sent to driver
20/07/15 10:48:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 827 ms on localhost (executor driver) (1/1)
20/07/15 10:48:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/07/15 10:48:59 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:44) finished in 1.022 s
20/07/15 10:48:59 INFO DAGScheduler: Job 0 finished: collect at utils.scala:44, took 1.083723 s
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 22
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 6
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 20
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 19
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 8
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 13
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 23
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 18
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 7
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 14
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 12
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 25
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 9
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 4
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 1
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 0
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 10
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 3
20/07/15 10:49:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on view-localhost:58804 in memory (size: 3.4 KB, free: 912.3 MB)
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 17
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 15
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 16
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 11
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 24
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 5
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 2
20/07/15 10:49:36 INFO ContextCleaner: Cleaned accumulator 21
20/07/15 10:49:37 INFO CodeGenerator: Code generated in 21.355 ms
20/07/15 10:49:37 INFO CodeGenerator: Code generated in 16.2525 ms
20/07/15 10:49:38 INFO SparkContext: Starting job: sql at <unknown>:0
20/07/15 10:49:38 INFO DAGScheduler: Registering RDD 13 (sql at <unknown>:0)
20/07/15 10:49:38 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
20/07/15 10:49:38 INFO DAGScheduler: Final stage: ResultStage 2 (sql at <unknown>:0)
20/07/15 10:49:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
20/07/15 10:49:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
20/07/15 10:49:38 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at sql at <unknown>:0), which has no missing parents
20/07/15 10:49:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.3 KB, free 912.3 MB)
20/07/15 10:49:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.7 KB, free 912.3 MB)
20/07/15 10:49:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on view-localhost:58804 (size: 10.7 KB, free: 912.3 MB)
20/07/15 10:49:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/07/15 10:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/07/15 10:49:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/07/15 10:49:40 INFO ContextCleaner: Cleaned accumulator 27
20/07/15 10:49:45 WARN TaskSetManager: Stage 1 contains a task of very large size (125392 KB). The maximum recommended task size is 100 KB.
20/07/15 10:49:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 128402017 bytes)
20/07/15 10:49:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/07/15 10:49:53 INFO CodeGenerator: Code generated in 18.2366 ms
20/07/15 10:49:53 INFO CodeGenerator: Code generated in 32.2519 ms
20/07/15 10:51:20 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 107.6 MB, free 804.7 MB)
20/07/15 10:51:20 INFO BlockManagerInfo: Added rdd_9_0 in memory on view-localhost:58804 (size: 107.6 MB, free: 804.7 MB)
20/07/15 10:51:20 INFO CodeGenerator: Code generated in 8.9352 ms
20/07/15 10:51:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2066 bytes result sent to driver
20/07/15 10:51:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 105030 ms on localhost (executor driver) (1/1)
20/07/15 10:51:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/07/15 10:51:23 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 105.059 s
20/07/15 10:51:23 INFO DAGScheduler: looking for newly runnable stages
20/07/15 10:51:23 INFO DAGScheduler: running: Set()
20/07/15 10:51:23 INFO DAGScheduler: waiting: Set(ResultStage 2)
20/07/15 10:51:23 INFO DAGScheduler: failed: Set()
20/07/15 10:51:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at sql at <unknown>:0), which has no missing parents
20/07/15 10:51:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.1 KB, free 804.7 MB)
20/07/15 10:51:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.8 KB, free 804.7 MB)
20/07/15 10:51:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on view-localhost:58804 (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:51:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
20/07/15 10:51:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/07/15 10:51:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/07/15 10:51:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/07/15 10:51:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
20/07/15 10:51:23 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/07/15 10:51:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
20/07/15 10:51:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1739 bytes result sent to driver
20/07/15 10:51:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 56 ms on localhost (executor driver) (1/1)
20/07/15 10:51:23 INFO DAGScheduler: ResultStage 2 (sql at <unknown>:0) finished in 0.075 s
20/07/15 10:51:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/07/15 10:51:23 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 105.186229 s
20/07/15 10:51:23 INFO CodeGenerator: Code generated in 7.287 ms
20/07/15 10:51:23 INFO SparkContext: Starting job: collect at utils.scala:114
20/07/15 10:51:23 INFO DAGScheduler: Registering RDD 20 (collect at utils.scala:114)
20/07/15 10:51:23 INFO DAGScheduler: Got job 2 (collect at utils.scala:114) with 1 output partitions
20/07/15 10:51:23 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:114)
20/07/15 10:51:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
20/07/15 10:51:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
20/07/15 10:51:23 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[20] at collect at utils.scala:114), which has no missing parents
20/07/15 10:51:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 23.3 KB, free 804.7 MB)
20/07/15 10:51:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.7 KB, free 804.7 MB)
20/07/15 10:51:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on view-localhost:58804 (size: 10.7 KB, free: 804.7 MB)
20/07/15 10:51:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
20/07/15 10:51:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[20] at collect at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:51:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 46
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 59
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 58
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 31
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 36
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 81
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 37
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 79
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 54
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 62
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 89
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 82
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 63
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 70
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 74
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 64
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 73
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 72
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 68
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 50
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 52
20/07/15 10:51:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on view-localhost:58804 in memory (size: 10.7 KB, free: 804.7 MB)
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 29
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 78
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 65
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 88
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 41
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 51
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 47
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 66
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 38
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 92
20/07/15 10:51:24 INFO ContextCleaner: Cleaned accumulator 33
20/07/15 10:51:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on view-localhost:58804 in memory (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 83
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 34
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 42
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 30
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 35
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 56
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 90
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 67
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 77
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 28
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 71
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 40
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 76
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 39
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 91
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 55
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 85
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 69
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 57
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 60
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 48
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 93
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 61
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 44
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 86
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 94
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 80
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 32
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 75
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 49
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 87
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 84
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 45
20/07/15 10:51:25 INFO ContextCleaner: Cleaned shuffle 0
20/07/15 10:51:25 INFO ContextCleaner: Cleaned accumulator 53
20/07/15 10:51:29 WARN TaskSetManager: Stage 3 contains a task of very large size (125392 KB). The maximum recommended task size is 100 KB.
20/07/15 10:51:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 128402017 bytes)
20/07/15 10:51:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/07/15 10:51:42 INFO BlockManager: Found block rdd_9_0 locally
20/07/15 10:51:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2109 bytes result sent to driver
20/07/15 10:51:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 18741 ms on localhost (executor driver) (1/1)
20/07/15 10:51:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/07/15 10:51:42 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:114) finished in 18.758 s
20/07/15 10:51:42 INFO DAGScheduler: looking for newly runnable stages
20/07/15 10:51:42 INFO DAGScheduler: running: Set()
20/07/15 10:51:42 INFO DAGScheduler: waiting: Set(ResultStage 4)
20/07/15 10:51:42 INFO DAGScheduler: failed: Set()
20/07/15 10:51:42 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at collect at utils.scala:114), which has no missing parents
20/07/15 10:51:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.1 KB, free 804.7 MB)
20/07/15 10:51:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 804.7 MB)
20/07/15 10:51:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on view-localhost:58804 (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:51:43 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
20/07/15 10:51:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at collect at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:51:43 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
20/07/15 10:51:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/07/15 10:51:43 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
20/07/15 10:51:43 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/07/15 10:51:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
20/07/15 10:51:43 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1696 bytes result sent to driver
20/07/15 10:51:43 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 13 ms on localhost (executor driver) (1/1)
20/07/15 10:51:43 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/07/15 10:51:43 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:114) finished in 0.579 s
20/07/15 10:51:43 INFO DAGScheduler: Job 2 finished: collect at utils.scala:114, took 19.352724 s
20/07/15 10:51:43 INFO CodeGenerator: Code generated in 12.6063 ms
20/07/15 10:51:43 INFO CodeGenerator: Code generated in 9.4544 ms
20/07/15 10:51:43 INFO SparkContext: Starting job: count at utils.scala:114
20/07/15 10:51:43 INFO DAGScheduler: Registering RDD 27 (count at utils.scala:114)
20/07/15 10:51:43 INFO DAGScheduler: Got job 3 (count at utils.scala:114) with 1 output partitions
20/07/15 10:51:43 INFO DAGScheduler: Final stage: ResultStage 6 (count at utils.scala:114)
20/07/15 10:51:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
20/07/15 10:51:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
20/07/15 10:51:43 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[27] at count at utils.scala:114), which has no missing parents
20/07/15 10:51:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.7 KB, free 804.7 MB)
20/07/15 10:51:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.6 KB, free 804.7 MB)
20/07/15 10:51:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on view-localhost:58804 (size: 9.6 KB, free: 804.7 MB)
20/07/15 10:51:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
20/07/15 10:51:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[27] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:51:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
20/07/15 10:51:44 INFO ContextCleaner: Cleaned accumulator 160
20/07/15 10:51:47 WARN TaskSetManager: Stage 5 contains a task of very large size (125392 KB). The maximum recommended task size is 100 KB.
20/07/15 10:51:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 128402017 bytes)
20/07/15 10:51:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 129
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 139
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 135
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 120
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 143
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 122
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 158
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 124
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 116
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 140
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 159
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 125
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 138
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 123
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 157
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 156
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 149
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 132
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 112
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 147
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 110
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 150
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 137
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 115
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 152
20/07/15 10:51:49 INFO BlockManagerInfo: Removed broadcast_3_piece0 on view-localhost:58804 in memory (size: 10.7 KB, free: 804.7 MB)
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 130
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 154
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 127
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 128
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 151
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 118
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 153
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 136
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 126
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 114
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 142
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 113
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 111
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 133
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 117
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 146
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 144
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 131
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 155
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 145
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 148
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 119
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 141
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 121
20/07/15 10:51:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on view-localhost:58804 in memory (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:51:49 INFO ContextCleaner: Cleaned accumulator 134
20/07/15 10:52:01 INFO BlockManager: Found block rdd_9_0 locally
20/07/15 10:52:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2109 bytes result sent to driver
20/07/15 10:52:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 17723 ms on localhost (executor driver) (1/1)
20/07/15 10:52:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/07/15 10:52:01 INFO DAGScheduler: ShuffleMapStage 5 (count at utils.scala:114) finished in 17.739 s
20/07/15 10:52:01 INFO DAGScheduler: looking for newly runnable stages
20/07/15 10:52:01 INFO DAGScheduler: running: Set()
20/07/15 10:52:01 INFO DAGScheduler: waiting: Set(ResultStage 6)
20/07/15 10:52:01 INFO DAGScheduler: failed: Set()
20/07/15 10:52:01 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[30] at count at utils.scala:114), which has no missing parents
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.1 KB, free 804.7 MB)
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.1 KB, free 804.7 MB)
20/07/15 10:52:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on view-localhost:58804 (size: 4.1 KB, free: 804.7 MB)
20/07/15 10:52:01 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[30] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:01 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
20/07/15 10:52:01 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/07/15 10:52:01 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
20/07/15 10:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/07/15 10:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/07/15 10:52:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2055 bytes result sent to driver
20/07/15 10:52:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 14 ms on localhost (executor driver) (1/1)
20/07/15 10:52:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/07/15 10:52:01 INFO DAGScheduler: ResultStage 6 (count at utils.scala:114) finished in 0.032 s
20/07/15 10:52:01 INFO DAGScheduler: Job 3 finished: count at utils.scala:114, took 17.823637 s
20/07/15 10:52:01 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:52:01 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:52:01 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:52:01 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:52:01 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/07/15 10:52:01 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/07/15 10:52:01 INFO CodeGenerator: Code generated in 8.8127 ms
20/07/15 10:52:01 INFO CodeGenerator: Code generated in 6.2853 ms
20/07/15 10:52:01 INFO CodeGenerator: Code generated in 8.5209 ms
20/07/15 10:52:01 INFO CodeGenerator: Code generated in 5.8373 ms
20/07/15 10:52:01 INFO SparkContext: Starting job: count at utils.scala:114
20/07/15 10:52:01 INFO DAGScheduler: Registering RDD 34 (count at utils.scala:114)
20/07/15 10:52:01 INFO DAGScheduler: Got job 4 (count at utils.scala:114) with 1 output partitions
20/07/15 10:52:01 INFO DAGScheduler: Final stage: ResultStage 8 (count at utils.scala:114)
20/07/15 10:52:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
20/07/15 10:52:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
20/07/15 10:52:01 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[34] at count at utils.scala:114), which has no missing parents
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.9 KB, free 804.7 MB)
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.2 KB, free 804.7 MB)
20/07/15 10:52:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on view-localhost:58804 (size: 4.2 KB, free: 804.7 MB)
20/07/15 10:52:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[34] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:01 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
20/07/15 10:52:01 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8017 bytes)
20/07/15 10:52:01 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
20/07/15 10:52:01 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1418 bytes result sent to driver
20/07/15 10:52:01 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 22 ms on localhost (executor driver) (1/1)
20/07/15 10:52:01 INFO DAGScheduler: ShuffleMapStage 7 (count at utils.scala:114) finished in 0.036 s
20/07/15 10:52:01 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/07/15 10:52:01 INFO DAGScheduler: looking for newly runnable stages
20/07/15 10:52:01 INFO DAGScheduler: running: Set()
20/07/15 10:52:01 INFO DAGScheduler: waiting: Set(ResultStage 8)
20/07/15 10:52:01 INFO DAGScheduler: failed: Set()
20/07/15 10:52:01 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at count at utils.scala:114), which has no missing parents
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.1 KB, free 804.7 MB)
20/07/15 10:52:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.8 KB, free 804.7 MB)
20/07/15 10:52:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on view-localhost:58804 (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:52:01 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
20/07/15 10:52:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/07/15 10:52:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
20/07/15 10:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/07/15 10:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/07/15 10:52:01 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1696 bytes result sent to driver
20/07/15 10:52:01 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 18 ms on localhost (executor driver) (1/1)
20/07/15 10:52:01 INFO DAGScheduler: ResultStage 8 (count at utils.scala:114) finished in 0.034 s
20/07/15 10:52:01 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/07/15 10:52:01 INFO DAGScheduler: Job 4 finished: count at utils.scala:114, took 0.094013 s
20/07/15 10:52:02 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:52:02 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:52:02 INFO HiveMetaStore: 0: get_database: default
20/07/15 10:52:02 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_database: default	
20/07/15 10:52:02 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/07/15 10:52:02 INFO audit: ugi=y14390	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/07/15 10:52:02 INFO SparkContext: Starting job: count at utils.scala:114
20/07/15 10:52:02 INFO DAGScheduler: Registering RDD 41 (count at utils.scala:114)
20/07/15 10:52:02 INFO DAGScheduler: Got job 5 (count at utils.scala:114) with 1 output partitions
20/07/15 10:52:02 INFO DAGScheduler: Final stage: ResultStage 10 (count at utils.scala:114)
20/07/15 10:52:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
20/07/15 10:52:02 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
20/07/15 10:52:02 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[41] at count at utils.scala:114), which has no missing parents
20/07/15 10:52:02 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.9 KB, free 804.7 MB)
20/07/15 10:52:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.2 KB, free 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 215
20/07/15 10:52:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on view-localhost:58804 (size: 4.2 KB, free: 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 108
20/07/15 10:52:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 185
20/07/15 10:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[41] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 274
20/07/15 10:52:02 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 297
20/07/15 10:52:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8017 bytes)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 184
20/07/15 10:52:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 109
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 218
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 290
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 97
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 284
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 245
20/07/15 10:52:02 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1461 bytes result sent to driver
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 201
20/07/15 10:52:02 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 21 ms on localhost (executor driver) (1/1)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 169
20/07/15 10:52:02 INFO DAGScheduler: ShuffleMapStage 9 (count at utils.scala:114) finished in 0.039 s
20/07/15 10:52:02 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
20/07/15 10:52:02 INFO DAGScheduler: looking for newly runnable stages
20/07/15 10:52:02 INFO ContextCleaner: Cleaned shuffle 2
20/07/15 10:52:02 INFO DAGScheduler: running: Set()
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 237
20/07/15 10:52:02 INFO DAGScheduler: waiting: Set(ResultStage 10)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 246
20/07/15 10:52:02 INFO DAGScheduler: failed: Set()
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 105
20/07/15 10:52:02 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[44] at count at utils.scala:114), which has no missing parents
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 96
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 183
20/07/15 10:52:02 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 165
20/07/15 10:52:02 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 295
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 193
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 289
20/07/15 10:52:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on view-localhost:58804 (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 287
20/07/15 10:52:02 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 282
20/07/15 10:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[44] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 95
20/07/15 10:52:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 241
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 281
20/07/15 10:52:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 301
20/07/15 10:52:02 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 197
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 283
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 298
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 232
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 166
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 223
20/07/15 10:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 251
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 234
20/07/15 10:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 194
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 268
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 249
20/07/15 10:52:02 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1696 bytes result sent to driver
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 276
20/07/15 10:52:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 28 ms on localhost (executor driver) (1/1)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 216
20/07/15 10:52:02 INFO DAGScheduler: ResultStage 10 (count at utils.scala:114) finished in 0.051 s
20/07/15 10:52:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
20/07/15 10:52:02 INFO DAGScheduler: Job 5 finished: count at utils.scala:114, took 0.121287 s
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 270
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 262
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 203
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 171
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 300
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 286
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 101
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 265
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 200
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 210
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 219
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 205
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 175
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 196
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 162
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 277
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 243
20/07/15 10:52:02 INFO BlockManagerInfo: Removed broadcast_6_piece0 on view-localhost:58804 in memory (size: 4.1 KB, free: 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 208
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 172
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 168
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 99
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 227
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 209
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 213
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 233
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 170
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 176
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 279
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 188
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 106
20/07/15 10:52:02 INFO BlockManagerInfo: Removed broadcast_8_piece0 on view-localhost:58804 in memory (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 263
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 182
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 235
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 240
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 294
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 238
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 224
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 211
20/07/15 10:52:02 INFO BlockManagerInfo: Removed broadcast_5_piece0 on view-localhost:58804 in memory (size: 9.6 KB, free: 804.7 MB)
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 214
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 299
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 288
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 206
20/07/15 10:52:02 INFO ContextCleaner: Cleaned accumulator 98
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 174
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 204
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 272
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 261
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 161
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 100
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 217
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 293
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 231
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 180
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 260
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 242
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 253
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 264
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 181
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 255
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 189
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 228
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 256
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 248
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 179
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 252
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 187
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 291
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 239
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 222
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 259
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 167
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 254
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 226
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 229
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 207
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 163
20/07/15 10:52:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on view-localhost:58804 in memory (size: 4.2 KB, free: 804.7 MB)
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 236
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 230
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 173
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 257
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 266
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 164
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 104
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 269
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 258
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 225
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 292
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 273
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 244
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 186
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 267
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 247
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 271
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 275
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 198
20/07/15 10:52:03 INFO ContextCleaner: Cleaned shuffle 3
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 212
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 202
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 178
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 107
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 285
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 192
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 190
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 102
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 280
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 103
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 195
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 191
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 177
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 199
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 250
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 221
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 296
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 278
20/07/15 10:52:03 INFO ContextCleaner: Cleaned accumulator 220
20/07/15 10:52:03 INFO ContextCleaner: Cleaned shuffle 1
20/07/15 10:52:03 INFO CodeGenerator: Code generated in 19.4827 ms
20/07/15 10:52:03 INFO Instrumentation: [4548f922] Stage class: ALS
20/07/15 10:52:03 INFO Instrumentation: [4548f922] Stage uid: als_2df82ec189a
20/07/15 10:52:03 INFO CodeGenerator: Code generated in 11.3623 ms
20/07/15 10:52:03 INFO Instrumentation: [4548f922] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
20/07/15 10:52:03 INFO Instrumentation: [4548f922] {"alpha":1.0,"ratingCol":"rating","numItemBlocks":10,"finalStorageLevel":"MEMORY_AND_DISK","implicitPrefs":false,"intermediateStorageLevel":"MEMORY_AND_DISK","nonnegative":false,"numUserBlocks":10,"rank":10,"itemCol":"item","checkpointInterval":10,"userCol":"user","regParam":0.1,"maxIter":10}
20/07/15 10:52:03 INFO SparkContext: Starting job: isEmpty at ALS.scala:919
20/07/15 10:52:03 INFO DAGScheduler: Got job 6 (isEmpty at ALS.scala:919) with 1 output partitions
20/07/15 10:52:03 INFO DAGScheduler: Final stage: ResultStage 11 (isEmpty at ALS.scala:919)
20/07/15 10:52:03 INFO DAGScheduler: Parents of final stage: List()
20/07/15 10:52:03 INFO DAGScheduler: Missing parents: List()
20/07/15 10:52:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[50] at map at ALS.scala:666), which has no missing parents
20/07/15 10:52:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 28.8 KB, free 804.7 MB)
20/07/15 10:52:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.9 KB, free 804.7 MB)
20/07/15 10:52:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on view-localhost:58804 (size: 12.9 KB, free: 804.7 MB)
20/07/15 10:52:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[50] at map at ALS.scala:666) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 355
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 344
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 326
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 322
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 357
20/07/15 10:52:04 INFO BlockManagerInfo: Removed broadcast_10_piece0 on view-localhost:58804 in memory (size: 3.8 KB, free: 804.7 MB)
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 366
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 333
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 354
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 362
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 346
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 336
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 352
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 347
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 335
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 338
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 332
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 341
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 321
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 339
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 331
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 337
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 323
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 343
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 348
20/07/15 10:52:04 INFO BlockManagerInfo: Removed broadcast_9_piece0 on view-localhost:58804 in memory (size: 4.2 KB, free: 804.7 MB)
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 356
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 340
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 364
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 303
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 334
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 342
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 330
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 329
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 353
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 365
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 359
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 363
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 318
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 320
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 358
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 351
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 361
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 324
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 327
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 360
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 328
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 319
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 325
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 350
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 345
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 367
20/07/15 10:52:04 INFO ContextCleaner: Cleaned accumulator 349
20/07/15 10:52:08 WARN TaskSetManager: Stage 11 contains a task of very large size (125392 KB). The maximum recommended task size is 100 KB.
20/07/15 10:52:08 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 128402028 bytes)
20/07/15 10:52:08 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 308
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 312
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 311
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 309
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 307
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 304
20/07/15 10:52:12 INFO ContextCleaner: Cleaned shuffle 4
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 314
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 306
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 302
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 315
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 313
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 316
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 310
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 305
20/07/15 10:52:12 INFO ContextCleaner: Cleaned accumulator 317
20/07/15 10:52:24 INFO BlockManager: Found block rdd_9_0 locally
20/07/15 10:52:27 INFO CodeGenerator: Code generated in 2296.2871 ms
20/07/15 10:52:27 INFO Executor: 1 block locks were not released by TID = 11:
[rdd_9_0]
20/07/15 10:52:27 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1641 bytes result sent to driver
20/07/15 10:52:27 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 23205 ms on localhost (executor driver) (1/1)
20/07/15 10:52:27 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
20/07/15 10:52:27 INFO DAGScheduler: ResultStage 11 (isEmpty at ALS.scala:919) finished in 23.232 s
20/07/15 10:52:27 INFO DAGScheduler: Job 6 finished: isEmpty at ALS.scala:919, took 23.257464 s
20/07/15 10:52:27 INFO SparkContext: Starting job: count at ALS.scala:932
20/07/15 10:52:27 INFO DAGScheduler: Registering RDD 56 (mapPartitions at ALS.scala:1322)
20/07/15 10:52:27 INFO DAGScheduler: Registering RDD 59 (map at ALS.scala:1565)
20/07/15 10:52:27 INFO DAGScheduler: Got job 7 (count at ALS.scala:932) with 10 output partitions
20/07/15 10:52:27 INFO DAGScheduler: Final stage: ResultStage 14 (count at ALS.scala:932)
20/07/15 10:52:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
20/07/15 10:52:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)
20/07/15 10:52:27 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[56] at mapPartitions at ALS.scala:1322), which has no missing parents
20/07/15 10:52:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 30.9 KB, free 804.7 MB)
20/07/15 10:52:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 13.8 KB, free 804.7 MB)
20/07/15 10:52:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on view-localhost:58804 (size: 13.8 KB, free: 804.7 MB)
20/07/15 10:52:27 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161
20/07/15 10:52:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[56] at mapPartitions at ALS.scala:1322) (first 15 tasks are for partitions Vector(0))
20/07/15 10:52:27 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 379
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 381
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 393
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 388
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 397
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 395
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 398
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 390
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 384
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 394
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 387
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 380
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 396
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 386
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 391
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 385
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 392
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 376
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 375
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 377
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 382
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 389
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 378
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 374
20/07/15 10:52:28 INFO BlockManagerInfo: Removed broadcast_11_piece0 on view-localhost:58804 in memory (size: 12.9 KB, free: 804.7 MB)
20/07/15 10:52:28 INFO ContextCleaner: Cleaned accumulator 383
20/07/15 10:52:32 WARN TaskSetManager: Stage 12 contains a task of very large size (125392 KB). The maximum recommended task size is 100 KB.
20/07/15 10:52:32 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 128402017 bytes)
20/07/15 10:52:32 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
20/07/15 10:52:46 INFO BlockManager: Found block rdd_9_0 locally
20/07/15 10:55:28 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 12)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
20/07/15 10:55:30 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 12,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
20/07/15 10:55:30 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 12, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

20/07/15 10:55:30 INFO SparkContext: Invoking stop() from shutdown hook
20/07/15 10:55:30 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job
20/07/15 10:55:30 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
20/07/15 10:55:30 INFO TaskSchedulerImpl: Cancelling stage 12
20/07/15 10:55:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage cancelled
20/07/15 10:55:30 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
20/07/15 10:55:30 INFO DAGScheduler: ShuffleMapStage 12 (mapPartitions at ALS.scala:1322) failed in 183.586 s due to Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
20/07/15 10:55:30 INFO DAGScheduler: Job 7 failed: count at ALS.scala:932, took 183.611558 s
20/07/15 10:55:30 ERROR Instrumentation: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
scala.Option.foreach(Option.scala:257)
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
org.apache.spark.rdd.RDD.count(RDD.scala:1168)
org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:932)
org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:676)
org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:658)
org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)
scala.util.Try$.apply(Try.scala:192)
org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)
org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:658)
org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:569)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
java.lang.reflect.Method.invoke(Unknown Source)
sparklyr.Invoke.invoke(invoke.scala:147)
sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
sparklyr.StreamHandler.read(stream.scala:61)
sparklyr.BackendHandler$$anonfun$channelRead0$1.apply$mcV$sp(handler.scala:58)
scala.util.control.Breaks.breakable(Breaks.scala:38)
sparklyr.BackendHandler.channelRead0(handler.scala:38)
sparklyr.BackendHandler.channelRead0(handler.scala:14)
io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)
io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
java.lang.Thread.run(Unknown Source)
20/07/15 10:55:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/07/15 10:55:30 INFO MemoryStore: MemoryStore cleared
20/07/15 10:55:30 INFO BlockManager: BlockManager stopped
20/07/15 10:55:30 INFO BlockManagerMaster: BlockManagerMaster stopped
20/07/15 10:55:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/07/15 10:55:30 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/07/15 10:55:30 INFO SparkContext: Successfully stopped SparkContext
20/07/15 10:55:30 INFO ShutdownHookManager: Shutdown hook called
20/07/15 10:55:30 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831
20/07/15 10:55:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/07/15 10:55:31 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\Temp\1\spark-fbe8b6a7-fd74-4c81-842c-3b1f15aa545e
20/07/15 10:55:31 INFO ShutdownHookManager: Deleting directory C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781
20/07/15 10:55:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781
java.io.IOException: Failed to delete: C:\Users\y14390\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-92424a26-9103-4be5-8c81-10a541a6f781\userFiles-8064ad23-cf74-482b-b1b3-534ec12dd831\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
