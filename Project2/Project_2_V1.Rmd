---
title: 'DATA 612: Project 2 | Content Based and Collaborative Filtering'
author: "Amber Ferger, Charlie Rosemond, Juanelle Marks" 
date: "6/9/2020"
output: 
    html_document:
     theme: lumen
     highlight: tango
     toc: TRUE
    
---

# Project Instructions

The goal of this assignment is for you to try out different ways of implementing and configuring a recommender, and to evaluate your different approaches.For assignment 2, start with an existing dataset of user-item ratings, such MovieLens. Implement at least two of these recommendation algorithms:

* Content-Based Filtering
* User-User Collaborative Filtering
* Item-Item Collaborative Filtering

You should evaluate and compare different approaches, using different algorithms, normalization techniques, similarity methods, neighborhood sizes, etc. You don’t need to be exhaustive—these are just some suggested possibilities.

You may use the course text’s recommenderlab or any other library that you want. Please provide at least one graph, and a textual summary of your findings and recommendations.

# Introduction and motivation
In this project we will use the built-in MovieLense dataset from the **recommenderlab** library. We will implement and compare results from several recommendation algorithms. Our recommenders will predict movie ratings given user and movie profiles as well as users' past ratings.


```{r message=FALSE, include=FALSE}
# load libraries
library(tidyverse)
library(pander)
library(knitr)
library(dplyr)
library(recommenderlab)
library(ggplot2)

```


### Load Data
Our data is stored as a *realRatingMatrix*, where each row represents a user and each column represents a movie. We can see that there are a total of 99,392 ratings, which means that not every user has watched every movie. This means that our matrix is sparse -- most cells have no information.
```{r}
set.seed(200)
data("MovieLense")
movielense <- MovieLense@data
show(MovieLense)

```

# Explore Data Set

With reference to code by (Hahsler, 2019), we did some exploration of the dataset so that we get get a sense of the how ratings are distributed according to user and item. This will inform how we will subset the data for modeling.


### Distribution of ratings by user and item
First, we can take a look at the number of ratings per user. 
```{r}
## number of ratings per user using base R
hist(rowCounts(MovieLense))
```

The data is severely right skewed, which indicates that most users have only rated a few movies.  
  
Next, we can take a look at the number of ratings per movie.

```{r}
## number of ratings per movie using base R
hist(colCounts(MovieLense))
```

This distribution is also severely right skewed, which means that most movies only have a few ratings. 

This aligns with our original intuitios -- given the skewedness in both of these distributions, it is obvious that there is a lot of sparsity in the dataset.

### Overall distribution of ratings
Next, we can look at the distribution of ratings.
```{r, warning=FALSE}
# Visualise the distribution of ratings in this dataset
movielense %>% 
  as.vector() %>% 
  as_tibble() %>% 
  filter_all(any_vars(. != 0)) %>% 
  ggplot(aes(value)) + 
  geom_bar(fill = 'red') +
  labs(title = " Overall Distribution of Ratings", y = "", x = "Ratings") +
  theme_minimal()
```
The overall distribution of ratings is left skewed, indicating that people tend to rate movies more positively. The mean rating is confirmed below:

```{r}
## mean rating (averaged over users)
mean(rowMeans(MovieLense))
```

### View of meta data and heatmap of ratings
The format of MovieLenseMeta is a data.frame with movie title, year, IMDb URL and indicator variables for 19 genres. See below view of meta data
```{r}
# View the meta data:
movie_meta <- MovieLenseMeta
str(movie_meta)
#pander(head(movie_meta), caption = "Sample Movie Meta Data")
```


We can also build out a heatmap of the most relevant users/movies. This will consist of the users that rated the most movies and the movies that have the most ratings. Dark columns represent highly rated movies and dark rows represent users that give high ratings. This indicates the need for normalization of the data -- we don't want to skew the results from users that predominantly rank high or low. 
```{r}
min_n_movies <- quantile(rowCounts(MovieLense), 0.99)
min_n_users <- quantile(colCounts(MovieLense), 0.99)

image(MovieLense[rowCounts(MovieLense) > min_n_movies,
colCounts(MovieLense) > min_n_users], main = "Heatmap of the top users
and movies")

```

# Prepare Data For Modeling

Our data prep will involve: selecting relevant data (movies that have a lot of ratings and users that have seen a lot of movies) and splitting into training and test sets. Once we've cleaned things up, we can start to build our recommender models. 

### Select relevant data

It is obvious that the dataset is very sparse. It includes many individuals that rated very few movies, and many movies that have very few ratings. In order to have a healthy baseline on which to our build recommendation models, we will take into consideration those users who have rated at least 50 movies and those movies that are rated by at least 100 users. A look at the disribution of average rating per user shows us that we have a large variation across the subset.  

```{r}
# base population
most_rated <- MovieLense[rowCounts(MovieLense) > 50, colCounts(MovieLense) > 100]
average_ratings_per_user <- rowMeans(most_rated)
qplot(average_ratings_per_user) + stat_bin(binwidth = 0.1) +
ggtitle("Distribution of the average rating per user")

```


### Split into training and testing
Next, we can split the data into train and test sets. We are left with 462 records in the training set and 98 records in the test set.

```{r}
# split the dataset into test set and train set
which_train <- sample(x = c(TRUE, FALSE), size = nrow(most_rated), replace = TRUE, prob = c(0.8, 0.2))
train_set <- most_rated[which_train, ]
test_set <- most_rated[!which_train, ]

paste0('Training Set: ',nrow(train_set), ' records | Test Set: ', nrow(test_set), ' records')

```

# Recommendation Algorithms
Now that we've cleaned up our data, we can configure and implement recommender systems based on 3 different filtering approaches: content-based, user-user collaborative, and item-item collaborative. 
  
In **content-based filtering**, recommendations are user-based -- a user profile is defined and the algorithm recommends new items that are similar to previously rated items. 

In **collaborative filtering**, recommendations are based on information about similar users (user-user) or similar items (item-item). The *recommenderlab* library contains 3 similarity fucntions that we will explore: cosine, pearson, and jaccard.

### Content-Based Filtering


### User-User Collaborative Filtering
**User-based collaborative filtering**: This recommends to a user the items that are the most preferred by similar users. Our approach will be to:

1. Measure the similarity between every pair of users.
2. Identify the most similar users.
3. Aggregate the ratings from the movies watched by similar users.
4. Select the top rated movies for recommendation. 

The parameters for this approach are similar to those of the item-item filtering: the similarity method (cosine, pearson, and jaccard), the number of nearest neighbors, and the normalization technique. 
  
We'll start off with a simple, non-optimized user-user based model to get a feel for the functions. We'll use our train set to create the model and our test set to get recommendations on. Here, we've defined the number of recommendations as 5, the similarity method as cosine, and the number of neighbors as 5. We'll plot the distribution of recommendations. 

```{r}
# recommend 5 movies to each user
n_recommended <- 5

# create model on training set
recc_model <- Recommender(data = train_set, method = "UBCF", parameter = list(method = 'cosine', nn = 5, normalize = 'center' ))

# predict on test set
recc_predict <- predict(object = recc_model, newdata = test_set, n = n_recommended)

# create a matrix of predictions
recc_matrix <- sapply(recc_predict@items, function(x){
colnames(most_rated)[x]
})

# plot distribution of movie recommendations
numRec <- factor(table(recc_matrix))

qplot(numRec,
      main = 'Distribution of Movie Recommendations',
      ylab='Number of Movies',
      xlab = 'Times Recommended') 

```

We can see that that most movies are recommended only a few times.  
  
Now, we can optimize our model by tweaking the similarity method, number of neighbors, and normalization type. We'll create a function to calculate and record the RMSE, MSE, and MAE for each set of parameters. 

```{r}
set.seed(200)

# function to calculate error 
errorCalc <- function(e, sm, nb, nrm, evl){
  
  # create a user-based CF recommender using training data
  rec <- Recommender(getData(e, "train"), 
                     evl, 
                     parameter = list(method = sm, 
                                      nn = nb, 
                                      normalize = nrm))
  
  # create predictions for the test data using known ratings 
  pred <- predict(rec, getData(e, "known"), type="ratings")
  
  # avg error metrics per user, avgd over all recommendations
  error.ubcf <-calcPredictionAccuracy(pred, getData(e, "unknown"))
  
  # evaluate topNLists instead 
  pred.top10 <- predict(rec, getData(e, "known"), type="topNList", n = 5)
  error.ubcf.top10 <- calcPredictionAccuracy(pred.top10,
                                             getData(e,"unknown"), 
                                             given=5, 
                                             goodRating=3)
  
  error.ubcf <- c(error.ubcf, sm, nb, nrm)
  names(error.ubcf) <- c('RMSE', 'MSE', 'MAE','similarity',
                         'neighbors','norm')
  
  return(error.ubcf)
}

```

In this case, we'll define an evaluation scheme that consists of 10-fold cross validation with a "good rating" defined as 3. We will cycle through all parameter combinations to identify what the optimal combination is. 
``` {r}
# cycle through all parameter combinations
simMethod <- c('cosine', 'pearson', 'jaccard')
neighbors <- seq(5,100,5)
norm <- c('center', 'Z-score')
evalType <- "UBCF"

# 10-fold cross validation, good ratings = 3, withholding 10 items for eval
e <- evaluationScheme(most_rated, method = "cross-validation", k=10, given = 10, goodRating=3)

# create list for errors
errors.names <- c("RMSE", "MSE", "MAE", 'similarity', 'neighbors','norm')
ubcfErrors.ls <- vector("list", length(errors.names))
names(ubcfErrors.ls) <- errors.names

for (i in simMethod){
  for (j in neighbors){
    for (k in norm){
      ubcfErrors.ls <- rbind(ubcfErrors.ls, errorCalc(e, i, j, k, evalType))
       
    }
  }
}

```

We can now visualize our errors: 
```{r}

finalErrors.ubcf <- as.data.frame(ubcfErrors.ls) %>%
  filter(RMSE != 'NULL') %>%
  mutate(RMSE = as.double(RMSE),
         MSE = as.double(MSE), 
         MAE = as.double(MAE)) 

finalErrors.ubcf$ERROR_MEAN<- rowMeans(subset(finalErrors.ubcf, select = c(RMSE, MSE, MAE)), na.rm = TRUE)

ggplot(finalErrors.ubcf, aes(1:nrow(finalErrors.ubcf))) +                    
  geom_line(aes(y=RMSE), colour="red") +  
  geom_line(aes(y=MSE), colour="green") + 
  geom_line(aes(y=MAE), colour="blue") +
  geom_line(aes(y=ERROR_MEAN), colour="purple") +
  xlab('Trial Run') +
  ylab('Error') +
  ggtitle('Errors across all runs')

```

To identify which set of parameters to use, we can identify the combination that has the lowest error.  

```{r}

paramVals.ubcf <- finalErrors.ubcf %>%
  filter(ERROR_MEAN == min(finalErrors.ubcf$ERROR_MEAN))

paramVals.ubcf

```

This is the set of parameters we will use for the final comparison. 


### Item-Item Collaborative Filtering
**Item-based collaborative filtering**: This recommends to a user the items that are most similar on the user's previous ratings. We will follow this process:

1. Measure the similarity in rating between every pair of movies.
2. Identify the k-most similar movies for each movie title. 
3. Identify the movies most similar to previous rated movies for each user. 

```{r}


```

### Evaluation

For our final evaluation, we'll take compare the user-user and item-item based modeling with the optimal parameters to each other. 
```{r}

results.ubcf <- evaluate(e, 'UBCF', parameter = list(method = 'cosine', 
                                      nn = 100, 
                                      normalize = 'center'))

avgResults.ubcf <- as.data.frame(avg(results.ubcf))
plot(avgResults.ubcf$FPR, avgResults.ubcf$TPR)

```




# Discussion
Pros of content-based systems:

* No need for data on other users, so you can start making recommendations right away for a user
* Can recommend to users with distinct taste (the users are very different to all other users)
* Can recommend new and unpopular items because we aren't relying on other users' ratings
* Recommendations are explainable because they're based on the user's current profile

Cons of content-based systems:

* Developing features can be difficult
* Does not recommend items outside of an individual's current profile
* Does not recommend items that the user may like but hasn't rated
* Cannot recommend items to brand new users with no information to make a user profile




# Conclusion

# References
Hahsler, M. (2019, August 27). MovieLense Dataset (100k). Rdrr.Io. https://rdrr.io/cran/recommenderlab/man/MovieLense.html  
  
“Recommender Systems 101 – a Step by Step Practical Example in R.” R, 24 Dec. 2014, www.r-bloggers.com/recommender-systems-101-a-step-by-step-practical-example-in-r/.

