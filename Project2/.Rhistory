library(tidyverse)
library(pander)
library(knitr)
library(dplyr)
library(recommenderlab)
library(ggplot2)
set.seed(200)
data("MovieLense")
movielense <- MovieLense@data
show(MovieLense)
hist(rowCounts(MovieLense))
hist(colCounts(MovieLense))
movielense %>%
as.vector() %>%
as_tibble() %>%
filter_all(any_vars(. != 0)) %>%
ggplot(aes(value)) +
geom_bar(fill = 'red') +
labs(title = " Overall Distribution of Ratings", y = "", x = "Ratings") +
theme_minimal()
mean(rowMeans(MovieLense))
movie_meta <- MovieLenseMeta
str(movie_meta)
min_n_movies <- quantile(rowCounts(MovieLense), 0.99)
min_n_users <- quantile(colCounts(MovieLense), 0.99)
image(MovieLense[rowCounts(MovieLense) > min_n_movies,
colCounts(MovieLense) > min_n_users], main = "Heatmap of the top users
and movies")
# base population
most_rated <- MovieLense[rowCounts(MovieLense) > 50, colCounts(MovieLense) > 100]
average_ratings_per_user <- rowMeans(most_rated)
qplot(average_ratings_per_user) + stat_bin(binwidth = 0.1) +
ggtitle("Distribution of the average rating per user")
which_train <- sample(x = c(TRUE, FALSE), size = nrow(most_rated), replace = TRUE, prob = c(0.8, 0.2))
train_set <- most_rated[which_train, ]
test_set <- most_rated[!which_train, ]
paste0('Training Set: ',nrow(train_set), ' records | Test Set: ', nrow(test_set), ' records')
n_recommended <- 5
# create model on training set
recc_model <- Recommender(data = train_set, method = "UBCF", parameter = list(method = 'cosine', nn = 5, normalize = 'center' ))
# predict on test set
recc_predict <- predict(object = recc_model, newdata = test_set, n = n_recommended)
# create a matrix of predictions
recc_matrix <- sapply(recc_predict@items, function(x){
colnames(most_rated)[x]
})
# plot distribution of movie recommendations
numRec <- factor(table(recc_matrix))
qplot(numRec,
main = 'Distribution of Movie Recommendations',
ylab='Number of Movies',
xlab = 'Times Recommended')
set.seed(200)
# function to calculate error
errorCalc <- function(e, sm, nb, nrm, evl){
# create a user-based CF recommender using training data
rec <- Recommender(getData(e, "train"),
evl,
parameter = list(method = sm,
nn = nb,
normalize = nrm))
# create predictions for the test data using known ratings
pred <- predict(rec, getData(e, "known"), type="ratings")
# avg error metrics per user, avgd over all recommendations
error.ubcf <-calcPredictionAccuracy(pred, getData(e, "unknown"))
# evaluate topNLists instead
pred.top10 <- predict(rec, getData(e, "known"), type="topNList", n = 5)
error.ubcf.top10 <- calcPredictionAccuracy(pred.top10,
getData(e,"unknown"),
given=5,
goodRating=3)
error.ubcf <- c(error.ubcf, sm, nb, nrm)
names(error.ubcf) <- c('RMSE', 'MSE', 'MAE','similarity',
'neighbors','norm')
return(error.ubcf)
}
# cycle through all parameter combinations
simMethod <- c('cosine', 'pearson', 'jaccard')
neighbors <- seq(5,100,5)
norm <- c('center', 'Z-score')
evalType <- "UBCF"
# 10-fold cross validation, good ratings = 3, withholding 10 items for eval
e <- evaluationScheme(most_rated, method = "cross-validation", k=10, given = 10, goodRating=3)
# create list for errors
errors.names <- c("RMSE", "MSE", "MAE", 'similarity', 'neighbors','norm')
ubcfErrors.ls <- vector("list", length(errors.names))
names(ubcfErrors.ls) <- errors.names
for (i in simMethod){
for (j in neighbors){
for (k in norm){
ubcfErrors.ls <- rbind(ubcfErrors.ls, errorCalc(e, i, j, k, evalType))
}
}
}
finalErrors.ubcf <- as.data.frame(ubcfErrors.ls) %>%
filter(RMSE != 'NULL') %>%
mutate(RMSE = as.double(RMSE),
MSE = as.double(MSE),
MAE = as.double(MAE))
finalErrors.ubcf$ERROR_MEAN<- rowMeans(subset(finalErrors.ubcf, select = c(RMSE, MSE, MAE)), na.rm = TRUE)
ggplot(finalErrors.ubcf, aes(1:nrow(finalErrors.ubcf))) +
geom_line(aes(y=RMSE), colour="red") +
geom_line(aes(y=MSE), colour="green") +
geom_line(aes(y=MAE), colour="blue") +
geom_line(aes(y=ERROR_MEAN), colour="purple") +
xlab('Trial Run') +
ylab('Error') +
ggtitle('Errors across all runs')
paramVals.ubcf <- finalErrors.ubcf %>%
filter(ERROR_MEAN == min(finalErrors.ubcf$ERROR_MEAN))
paramVals.ubcf
n_recommended_i <- 5
# create model on training set
recc_model_i <- Recommender(data = train_set, method = "IBCF", parameter = list(method = 'cosine', k = 5, normalize = 'center'))
# predict on test set
recc_predict_i <- predict(object = recc_model_i, newdata = test_set, n = n_recommended_i)
# create a matrix of predictions
recc_matrix_i <- sapply(recc_predict_i@items, function(x){
colnames(most_rated)[x]
})
# plot distribution of movie recommendations
numRec_i <- factor(table(recc_matrix_i))
qplot(numRec_i,
main = 'Distribution of Movie Recommendations',
ylab='Number of Movies',
xlab = 'Times Recommended')
errorCalc_i <- function(e, sm, kn, nrm, evl){
set.seed(200)
# create a user-based CF recommender using training data
rec <- Recommender(getData(e, "train"), evl, parameter = list(method = sm, k = kn, normalize = nrm))
# create predictions for the test data using known ratings
pred <- predict(rec, getData(e, "known"), type="ratings")
# avg error metrics per user, avgd over all recommendations
error.ibcf <-calcPredictionAccuracy(pred, getData(e, "unknown"))
error.ibcf <- c(error.ibcf, sm, kn, nrm)
names(error.ibcf) <- c('RMSE', 'MSE', 'MAE','similarity', 'neighbor items', 'norm')
return(error.ibcf)
}
simMethod <- c('cosine', 'pearson', 'jaccard')
vector_k <- seq(5, 100, 5)
norm <- c('center', 'Z-score')
evalType <- "IBCF"
# 10-fold cross validation, good ratings = 3, withholding 10 items for eval
e <- evaluationScheme(most_rated, method = "cross-validation", k=10, given = 10, goodRating=3)
# create list for errors
errors.names <- c("RMSE", "MSE", "MAE", 'similarity', 'k-closest items', 'norm')
ibcfErrors.ls <- vector("list", length(errors.names))
names(ibcfErrors.ls) <- errors.names
for (i in simMethod) {
for (j in vector_k) {
for (k in norm){
ibcfErrors.ls <- rbind(ibcfErrors.ls, errorCalc_i(e, i, j, k, evalType))
}
}
}
finalErrors.ibcf <- as.data.frame(ibcfErrors.ls) %>%
filter(RMSE != 'NULL') %>%
mutate(RMSE = as.double(RMSE),
MSE = as.double(MSE),
MAE = as.double(MAE))
# Adding a column containing mean error
finalErrors.ibcf$ERROR_MEAN<- rowMeans(subset(finalErrors.ibcf, select = c(RMSE, MSE, MAE)), na.rm = TRUE)
# Plotting the errors
ggplot(finalErrors.ibcf, aes(1:nrow(finalErrors.ibcf))) +
geom_line(aes(y=RMSE), colour="red") +
geom_line(aes(y=MSE), colour="green") +
geom_line(aes(y=MAE), colour="blue") +
geom_line(aes(y=ERROR_MEAN), colour="purple") +
xlab('Trial Run') +
ylab('Error') +
ggtitle('Errors across all runs')
# Finding parameter values that minimize the mean error
paramVals.ibcf <- finalErrors.ibcf %>%
filter(ERROR_MEAN == min(finalErrors.ibcf$ERROR_MEAN))
paramVals.ibcf
# Results of model that minimizes mean error
results.ubcf <- evaluate(e, 'UBCF', parameter = list(method = 'cosine',
nn = 100,
normalize = 'center'))
# Plotting True Positive Rate (TPR) against False Positive Rate (FPR)
avgResults.ubcf <- as.data.frame(avg(results.ubcf))
plot(avgResults.ubcf$FPR, avgResults.ubcf$TPR)
# Plotting Precision-recall
plot(results.ubcf, "prec/rec", annotate = TRUE, main = "Precision-recall")
# Plotting True Positive Rate (TPR) against False Positive Rate (FPR)
avgResults.ubcf <- as.data.frame(avg(results.ubcf))
plot(avgResults.ubcf$FPR, avgResults.ubcf$TPR, annotate = TRUE, main = 'ROC Analysis - UBCF')
# Plotting Precision-recall
plot(results.ubcf, "prec/rec", annotate = TRUE, main = "Precision-recall - UBCF")
plot(avgResults.ubcf, "FPR/TPR", annotate = TRUE, main = 'ROC Analysis - UBCF')
# Plotting True Positive Rate (TPR) against False Positive Rate (FPR)
avgResults.ubcf <- as.data.frame(avg(results.ubcf))
plot(avgResults.ubcf$FPR, avgResults.ubcf$TPR, annotate = TRUE, main = 'ROC Analysis - UBCF')
# Plotting Precision-recall
plot(results.ubcf, "prec/rec", annotate = TRUE, main = "Precision-recall - UBCF")
plot(avg(results.ubcf),'FPR/TPR', annotate = TRUE, main = 'ROC Analysis - UBCF')
avgResults.ubcf <- as.data.frame(avg(results.ubcf))
plot(avgResults.ubcf$FPR, avgResults.ubcf$TPR, annotate = TRUE, main = 'ROC Analysis - UBCF')
# Plotting Precision-recall
plot(results.ubcf, "prec/rec", annotate = TRUE, main = "Precision-recall - UBCF")
# Evaluating using the parameter values - Pearson's, k = 100, Z-score normalization
results.ibcf <- evaluate(e, 'IBCF', parameter = list(method = paramVals.ibcf[[4]],
k = as.integer(paramVals.ibcf[[5]]),
normalize = paramVals.ibcf[[6]]))
# Plotting ROC curve and Precision-recall
plot(results.ibcf, annotate = TRUE, main = "ROC curve")
plot(results.ibcf, "prec/rec", annotate = TRUE, main = "Precision-recall")
plot(results.ubcf, annotate = TRUE, main = "ROC curve")
models_to_evaluate <- list(results.ubcf = list(name = "UBCF", param = list(method = 'cosine',
nn = 100,
normalize = 'center')),
results.ibcf = list(name = "IBCF", param = list(method = paramVals.ibcf[[4]],
k = as.integer(paramVals.ibcf[[5]]),
normalize = paramVals.ibcf[[6]])))
# Evaluating at various numbers of recommendations
n_recommendations <- seq(5, 100, 5)
list_results <- evaluate(e, method = models_to_evaluate, n = n_recommendations)
plot(list_results, legend = "topleft", main = "ROC curve")
plot(list_results, "prec/rec", legend = "bottomright", main = "Precision-recall")
plot(results.ubcf, "prec/rec", annotate = TRUE, main = "Precision-recall - UBCF")
results.ibcf <- evaluate(e, 'IBCF', parameter = list(method = paramVals.ibcf[[4]],
k = as.integer(paramVals.ibcf[[5]]),
normalize = paramVals.ibcf[[6]]))
# Plotting ROC curve and Precision-recall
plot(results.ibcf, annotate = TRUE, main = "ROC curve")
plot(results.ibcf, "prec/rec", annotate = TRUE, main = "Precision-recall")
# Plotting and comparing ROC curve and Precision-recall for each model
plot(list_results, legend = "topleft", main = "ROC curve")
plot(list_results, "prec/rec", legend = "bottomright", main = "Precision-recall")
